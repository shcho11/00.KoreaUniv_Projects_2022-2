{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi116GxmxoYfQ4oC2VChXj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shcho11/00.Projects_KoreaUniv_2022-2/blob/main/202212_RME_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMtKS1fNu-AJ",
        "outputId": "057e7887-1254-4b41-8100-bd5a828876b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install text_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPMv9vqVzB2c",
        "outputId": "1d76ce2e-98c2-4d99-e2df-e59d55ea58bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting text_utils\n",
            "  Downloading text_utils-0.0.5-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from text_utils) (1.21.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from text_utils) (4.6.0.66)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from text_utils) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->text_utils) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->text_utils) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->text_utils) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->text_utils) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->text_utils) (1.15.0)\n",
            "Installing collected packages: text-utils\n",
            "Successfully installed text-utils-0.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-text-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88yknbNXUEHc",
        "outputId": "d61c0e9b-226e-4ed5-f53c-a16fd6f7a580"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-text-utils\n",
            "  Downloading python_text_utils-0.0.6-py2.py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from python-text-utils) (1.15.0)\n",
            "Installing collected packages: python-text-utils\n",
            "Successfully installed python-text-utils-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install constants"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNuIE1KPcdD7",
        "outputId": "70656d30-4a73-4ab4-c975-3f8c545a3c34"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting constants\n",
            "  Downloading constants-0.6.0.tar.gz (5.1 kB)\n",
            "Building wheels for collected packages: constants\n",
            "  Building wheel for constants (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for constants: filename=constants-0.6.0-py3-none-any.whl size=5473 sha256=14ba023dc6dad6f92a8b949fc0c9215b2440ef478b27b2575e8c78ad4c0510a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/ac/b2/89268490b92bf6fd0102b3634668042437e0e024c64ef447a1\n",
            "Successfully built constants\n",
            "Installing collected packages: constants\n",
            "Successfully installed constants-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wmf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zv_sbYLeQRB",
        "outputId": "cd84ad6d-6f84-4db2-c59c-3736dc70d6a4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wmf\n",
            "  Downloading wmf-0.4.2.tar.gz (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 4.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wmf\n",
            "  Building wheel for wmf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wmf: filename=wmf-0.4.2-cp38-cp38-linux_x86_64.whl size=585614 sha256=7724902140f8adb91e395167a541b5a79727fabc8f8df70e81765f3d9235838d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/39/3d/8eceaf95902c25ab77105ef03a69fb755af6192b682f26a2f0\n",
            "Successfully built wmf\n",
            "Installing collected packages: wmf\n",
            "Successfully installed wmf-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOtG9tLMegzt",
        "outputId": "5c689ab5-05a2-4ad0-f545-9ea34d4ec11a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyll\n",
            "  Downloading Pyll-0.4.3.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting Paste>=1.7.5.1\n",
            "  Using cached Paste-3.5.2-py2.py3-none-any.whl (593 kB)\n",
            "Collecting PasteScript>=1.7.5\n",
            "  Using cached PasteScript-3.2.1-py2.py3-none-any.whl (73 kB)\n",
            "Collecting Beaker>=1.6.4\n",
            "  Downloading Beaker-1.12.0.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting FormEncode>=1.2.4\n",
            "  Downloading FormEncode-2.0.1-py2.py3-none-any.whl (363 kB)\n",
            "\u001b[K     |████████████████████████████████| 363 kB 59.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.6 in /usr/local/lib/python3.8/dist-packages (from pyll) (2.11.3)\n",
            "Collecting Genshi>=0.6\n",
            "  Downloading Genshi-0.7.7-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 68.1 MB/s \n",
            "\u001b[?25hCollecting WebError>=0.10.3\n",
            "  Downloading WebError-0.13.1.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting WebHelpers>=1.3\n",
            "  Downloading WebHelpers-1.3.tar.gz (729 kB)\n",
            "\u001b[K     |████████████████████████████████| 729 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psycopg2>=2.4 in /usr/local/lib/python3.8/dist-packages (from pyll) (2.9.5)\n",
            "Collecting transaction>=1.3\n",
            "  Downloading transaction-3.0.1-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting zope.sqlalchemy>=0.7.1\n",
            "  Downloading zope.sqlalchemy-1.6-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: Pygments>=1.5 in /usr/local/lib/python3.8/dist-packages (from pyll) (2.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.15 in /usr/local/lib/python3.8/dist-packages (from pyll) (2.0.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from pyll) (4.4.2)\n",
            "Requirement already satisfied: SQLAlchemy>=0.7.9 in /usr/local/lib/python3.8/dist-packages (from pyll) (1.4.44)\n",
            "Collecting WebOb>=0.5.1\n",
            "  Downloading WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 60.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from FormEncode>=1.2.4->pyll) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from Paste>=1.7.5.1->pyll) (57.4.0)\n",
            "Collecting PasteDeploy\n",
            "  Using cached PasteDeploy-3.0.1-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy>=0.7.9->pyll) (2.0.1)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (261 kB)\n",
            "\u001b[K     |████████████████████████████████| 261 kB 29.3 MB/s \n",
            "\u001b[?25hCollecting Tempita\n",
            "  Downloading Tempita-0.5.2-py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: pyll, Beaker, WebError, WebHelpers\n",
            "  Building wheel for pyll (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyll: filename=Pyll-0.4.3-py3-none-any.whl size=1188720 sha256=f3929b9436aa87a1f72c0b90aa46e33d443b731785a2773202985995f537324a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/da/d1/e7caf67fc14158a6fbc43f5e6d822516162bb5efbffb0ed6ce\n",
            "  Building wheel for Beaker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Beaker: filename=Beaker-1.12.0-py3-none-any.whl size=51440 sha256=78857ccd0c7d564c0e4e4e108a7c7357d7f6eec3af1455b3c31b41afc01291ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/e2/06/b45fb00cf10607a47e1e4be803a748a958fd1bb558d0bead60\n",
            "  Building wheel for WebError (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for WebError: filename=WebError-0.13.1-py3-none-any.whl size=89449 sha256=8c5f78cf327a0c29c2200d2fd6666c0a4f836cf035d007f0d5e7e20f2fcadc85\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/3f/39/9b98ffb5e39b4325d4404a1b92a29649563da7f80907076fb8\n",
            "  Building wheel for WebHelpers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for WebHelpers: filename=WebHelpers-1.3-py3-none-any.whl size=149051 sha256=828aaa3ccc2844bb3ea41061e27840d8e13eae07f3ce9db340a08303269581f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/03/48/7bb543e415b4fc845d555dbb517180570a1671caa3c861afbf\n",
            "Successfully built pyll Beaker WebError WebHelpers\n",
            "Installing collected packages: zope.interface, WebOb, transaction, Tempita, PasteDeploy, Paste, zope.sqlalchemy, WebHelpers, WebError, PasteScript, Genshi, FormEncode, Beaker, pyll\n",
            "Successfully installed Beaker-1.12.0 FormEncode-2.0.1 Genshi-0.7.7 Paste-3.5.2 PasteDeploy-3.0.1 PasteScript-3.2.1 Tempita-0.5.2 WebError-0.13.1 WebHelpers-1.3 WebOb-1.8.7 pyll-0.4.3 transaction-3.0.1 zope.interface-5.5.2 zope.sqlalchemy-1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install produce_negative_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4scgWkteZE-",
        "outputId": "6120ddf3-5c0d-4cda-d5d5-8eaeae1e6961"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package produce_negative_embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "import numpy as np\n",
        "import time\n",
        "import python_text_utils\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.utils import shuffle\n",
        "from joblib import Parallel, delayed\n",
        "import constants as gc\n",
        "\n",
        "np.random.seed(98765) #set random seed\n",
        "\n",
        "import argparse"
      ],
      "metadata": {
        "id": "dxjzh0OvyyNj"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(\"Description: Running multi-embedding recommendation - RME model\")\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument('--data_path', default='data', type=str, help='path to the data')\n",
        "parser.add_argument('--batch_size', default=5000, type=float, help='batch processing')\n",
        "parser.add_argument('--dataset', default=\"ml10m\", type=str, help='dataset')\n",
        "args = parser.parse_args()\n"
      ],
      "metadata": {
        "id": "988k3ZY8yygW"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PFtIOlN44bv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tpWW7z55yymD",
        "outputId": "b44a493b-5d8a-4056-f5f5-673ba73dec14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR =  os.path.join('drive', 'MyDrive', 'dl2022')\n",
        "\n",
        "unique_uid = list()\n",
        "with open(os.path.join(DATA_DIR, 'unique_uid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_uid.append(line.strip())\n",
        "\n",
        "unique_movieId = list()\n",
        "with open(os.path.join(DATA_DIR, 'unique_sid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_movieId.append(line.strip())\n",
        "n_items = len(unique_movieId)\n",
        "n_users = len(unique_uid)\n",
        "\n",
        "print('number of users:%d ,  number of items: %d'%(n_users, n_items))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIzTSCHXyyjA",
        "outputId": "1aab5194-ad7e-4244-c298-79150df6c02e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of users:20279 ,  number of items: 6051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(csv_file, shape=(n_users, n_items)):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    rows, cols = np.array(tp['userId']), np.array(tp['movieId']) #rows will be user ids, cols will be item-ids.\n",
        "    seq = np.concatenate((  rows[:, None], cols[:, None], np.ones((rows.size, 1), dtype='int')\n",
        "                          ), axis=1)\n",
        "    data = sparse.csr_matrix((np.ones_like(rows), (rows, cols)), dtype=np.int16, shape=shape)\n",
        "    return data, seq, tp"
      ],
      "metadata": {
        "id": "8ffprJvWyyo_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################Generate item-item co-occurrence matrix based on the user's consumed items history ############\n",
        "# ##################       This will build a item-item co-occurrence matrix           ############################\n",
        "#user 1: item 1, item 2, ... item k --> item 1, 2, ..., k will be seen as a sentence ==> do co-occurrence.\n",
        "\n",
        "def _coord_batch(lo, hi, train_data, prefix = 'item', max_neighbor_words = 100000, choose='macro'):\n",
        "    rows = []\n",
        "    cols = []\n",
        "\n",
        "    for u in range(lo, hi):\n",
        "        #print train_data[u].nonzero()[1] #names all the item ids that the user at index u watched nonzero return a\n",
        "        # 2D array, index 0 will be the row index and index 1 will be columns whose values are not equal to 0\n",
        "        lst_words = train_data[u].nonzero()[1]\n",
        "        if len(lst_words) > max_neighbor_words:\n",
        "            if choose == 'micro':\n",
        "                #approach 1: randomly select max_neighbor_words for each word.\n",
        "                for w in lst_words:\n",
        "                    tmp = lst_words.remove(w)\n",
        "                    #random choose max_neigbor words in the list:\n",
        "                    neighbors = np.random.choice(tmp, max_neighbor_words, replace=False)\n",
        "                    for c in neighbors:\n",
        "                        rows.append(w)\n",
        "                        cols.append(c)\n",
        "            if choose == 'macro':\n",
        "                #approach 2: randomly select the sentence with length of max_neigbor_words + 1, then do permutation.\n",
        "                lst_words = np.random.choice(lst_words, max_neighbor_words + 1, replace=False)\n",
        "                for w, c in itertools.permutations(lst_words, 2):\n",
        "                    rows.append(w)\n",
        "                    cols.append(c)\n",
        "        else:\n",
        "            for w, c in itertools.permutations(lst_words, 2):\n",
        "                rows.append(w)\n",
        "                cols.append(c)\n",
        "    if not os.path.exists(os.path.join(DATA_DIR, 'co-temp')): os.mkdir(os.path.join(DATA_DIR, 'co-temp'))\n",
        "    np.save(os.path.join(DATA_DIR, 'co-temp' ,'%s_coo_%d_%d.npy' % (prefix, lo, hi)),\n",
        "            np.concatenate([np.array(rows)[:, None], np.array(cols)[:, None]], axis=1)) #append column wise.\n",
        "    pass\n",
        "\n",
        "\n",
        "batch_size = args.batch_size\n",
        "\n",
        "train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train.csv'))\n",
        "#clear the co-temp folder:\n",
        "if os.path.exists(os.path.join(DATA_DIR, 'co-temp')):\n",
        "    for f in glob.glob(os.path.join(DATA_DIR, 'co-temp', '*.npy')):\n",
        "        os.remove(f)\n",
        "\n",
        "GENERATE_ITEM_ITEM_COOCCURENCE_FILE = True\n",
        "if GENERATE_ITEM_ITEM_COOCCURENCE_FILE:\n",
        "    t1 = time.time()\n",
        "    print ('Generating item item co-occurrence matrix')\n",
        "    start_idx = range(0, n_users, batch_size)\n",
        "    list_start_idx = list(start_idx)\n",
        "    end_idx = list_start_idx[1:] + [n_users]\n",
        "    Parallel(n_jobs=1)(delayed(_coord_batch)(lo, hi, train_data, prefix = 'item', max_neighbor_words = 200) for lo, hi in zip(start_idx, end_idx))\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds'%(t2-t1))\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5S9nC2r5KQf",
        "outputId": "9051761e-0a6c-4363-e9e5-48d7f9df4a95"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating item item co-occurrence matrix\n",
            "Time : 48 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "def save_pickle(data, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
        "        print ('Saved %s..' %path)\n",
        "\n",
        "# load\n",
        "def load_pickle(path):\n",
        "   with open(path, 'rb') as f:\n",
        "      pickle.load(f)"
      ],
      "metadata": {
        "id": "bZNL1psDWYW-"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################################################################\n",
        "####################Generate user-user co-occurrence matrix based on the same items they backed######################\n",
        "#####################        This will build a user-user co-occurrence matrix ##########################################\n",
        "\n",
        "GENERATE_USER_USER_COOCCURENCE_FILE = True\n",
        "if GENERATE_USER_USER_COOCCURENCE_FILE:\n",
        "    t1 = time.time()\n",
        "    print('Generating user user co-occurrence matrix')\n",
        "    start_idx = range(0, n_items, batch_size)\n",
        "    list_start_idx = list(start_idx)\n",
        "    end_idx = list_start_idx[1:] + [n_items]\n",
        "    Parallel(n_jobs=8)(delayed(_coord_batch)(lo, hi, train_data.T, prefix = 'user', max_neighbor_words=100) for lo, hi in zip(start_idx, end_idx))\n",
        "    t2 = time.time()\n",
        "    print('Time : %d seconds'%(t2 - t1))\n",
        "    pass\n",
        "########################################################################################################################\n",
        "\n",
        "def _load_coord_matrix(start_idx, end_idx, nrow, ncol, prefix = 'item'):\n",
        "    X = sparse.csr_matrix((nrow, ncol), dtype='float32')\n",
        "\n",
        "    for lo, hi in zip(start_idx, end_idx):\n",
        "        coords = np.load(os.path.join(DATA_DIR, 'co-temp', '%s_coo_%d_%d.npy' % (prefix, lo, hi)))\n",
        "\n",
        "        rows = coords[:, 0]\n",
        "        cols = coords[:, 1]\n",
        "\n",
        "        tmp = sparse.coo_matrix((np.ones_like(rows), (rows, cols)), shape=(nrow, ncol), dtype='float32').tocsr()\n",
        "        X = X + tmp\n",
        "\n",
        "        print(\"%s %d to %d finished\" % (prefix, lo, hi))\n",
        "        sys.stdout.flush()\n",
        "    return X\n",
        "\n",
        "BOOLEAN_LOAD_PP_COOCC_FROM_FILE = True\n",
        "X, Y = None, None\n",
        "if BOOLEAN_LOAD_PP_COOCC_FROM_FILE:\n",
        "    print ('Loading item item co-occurrence matrix')\n",
        "    t1 = time.time()\n",
        "    start_idx = range(0, n_users, batch_size)\n",
        "    list_start_idx = list(start_idx)\n",
        "    end_idx = list_start_idx[1:] + [n_users]\n",
        "    X = _load_coord_matrix(start_idx, end_idx, n_items, n_items, prefix = 'item') #item item co-occurrence matrix\n",
        "    print ('dumping matrix ...')\n",
        "    save_pickle(X, os.path.join(DATA_DIR,'item_item_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds'%(t2-t1))\n",
        "else:\n",
        "    print ('test loading model from pickle file')\n",
        "    t1 = time.time()\n",
        "    X = load_pickle(os.path.join(DATA_DIR,'item_item_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('[INFO]: sparse matrix size of item-item co-occurrence matrix: %d mb\\n' % (\n",
        "                                                    (X.data.nbytes + X.indices.nbytes + X.indptr.nbytes) / (1024 * 1024)))\n",
        "    print ('Time : %d seconds'%(t2-t1))\n",
        "\n",
        "#X = None\n",
        "BOOLEAN_LOAD_UU_COOCC_FROM_FILE = True\n",
        "if BOOLEAN_LOAD_UU_COOCC_FROM_FILE:\n",
        "    print ('Loading user user co-occurrence matrix')\n",
        "    t1 = time.time()\n",
        "    start_idx = range(0, n_items, batch_size)\n",
        "    list_start_idx = list(start_idx)\n",
        "    end_idx = list_start_idx[1:] + [n_items]\n",
        "    Y = _load_coord_matrix(start_idx, end_idx, n_users, n_users, prefix = 'user') #user user co-occurrence matrix\n",
        "\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "    print ('dumping matrix ...')\n",
        "    t1 = time.time()\n",
        "    save_pickle(Y, os.path.join(DATA_DIR, 'user_user_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds'%(t2-t1))\n",
        "else:\n",
        "    print ('test loading model from pickle file')\n",
        "    t1 = time.time()\n",
        "    Y = load_pickle(os.path.join(DATA_DIR, 'user_user_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('[INFO]: sparse matrix size of user user co-occurrence matrix: %d mb\\n' % (\n",
        "                                                    (Y.data.nbytes + Y.indices.nbytes + Y.indptr.nbytes) / (1024 * 1024)))\n",
        "    print ('Time : %d seconds'%(t2-t1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEdnQEl35KTH",
        "outputId": "1f98f727-cb3b-404c-f9b8-f9fca918bfd3"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating user user co-occurrence matrix\n",
            "Time : 33 seconds\n",
            "Loading item item co-occurrence matrix\n",
            "item 0 to 5000 finished\n",
            "item 5000 to 10000 finished\n",
            "item 10000 to 15000 finished\n",
            "item 15000 to 20000 finished\n",
            "item 20000 to 20279 finished\n",
            "dumping matrix ...\n",
            "Saved drive/MyDrive/dl2022/item_item_cooc.dat..\n",
            "Time : 12 seconds\n",
            "Loading user user co-occurrence matrix\n",
            "user 0 to 5000 finished\n",
            "user 5000 to 6051 finished\n",
            "Time : 2 seconds\n",
            "dumping matrix ...\n",
            "Saved drive/MyDrive/dl2022/user_user_cooc.dat..\n",
            "Time : 0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################Generate item-item co-occurrence matrix based on the user backed items history ############\n",
        "# ##################       This will build a item-item co-occurrence matrix           ############################\n",
        "#user 1: item 1, item 2, ... item k --> item 1, 2, ..., k will be seen as a sentence ==> do co-occurrence.\n",
        "\n",
        "def _coord_batch_n(lo, hi, train_data, prefix = 'item', max_neighbor_words = 200, choose='macro'):\n",
        "    rows = []\n",
        "    cols = []\n",
        "\n",
        "    for u in range(lo, hi):\n",
        "        #print train_data[u].nonzero()[1] #names all the item ids that the user at index u watched nonzero return a\n",
        "        # 2D array, index 0 will be the row index and index 1 will be columns whose values are not equal to 0\n",
        "        lst_words = train_data[u].nonzero()[1]\n",
        "        #some users may have some hundreds of consumed items\n",
        "        # --> It's terrible time consuming when perfoming permutations of this long sequence. --> limit max_neighbor_words\n",
        "        # --> need to randomly pickup max_neighbor_words when the total of consumed items is > max_neighbor_words.\n",
        "        if len(lst_words) > max_neighbor_words:\n",
        "            if choose == 'micro':\n",
        "                #approach 1: randomly select max_neighbor_words for each word.\n",
        "                for w in lst_words:\n",
        "                    tmp = lst_words.remove(w)\n",
        "                    #random choose max_neigbor words in the list:\n",
        "                    neighbors = np.random.choice(tmp, max_neighbor_words, replace=False)\n",
        "                    for c in neighbors:\n",
        "                        rows.append(w)\n",
        "                        cols.append(c)\n",
        "            if choose == 'macro':\n",
        "                #approach 2: randomly select the sentence with length of max_neigbor_words + 1, then do permutation.\n",
        "                lst_words = np.random.choice(lst_words, max_neighbor_words + 1, replace=False)\n",
        "                for w, c in itertools.permutations(lst_words, 2):\n",
        "                    rows.append(w)\n",
        "                    cols.append(c)\n",
        "        else:\n",
        "\n",
        "            for w, c in itertools.permutations(lst_words, 2):\n",
        "                rows.append(w)\n",
        "                cols.append(c)\n",
        "    if not os.path.exists(os.path.join(DATA_DIR, 'negative-co-temp')): os.mkdir(os.path.join(DATA_DIR, 'negative-co-temp'))\n",
        "    np.save(os.path.join(DATA_DIR, 'negative-co-temp' ,'negative_%s_coo_%d_%d.npy' % (prefix, lo, hi)),\n",
        "            np.concatenate([np.array(rows)[:, None], np.array(cols)[:, None]], axis=1)) #append column wise.\n",
        "    pass\n",
        "\n",
        "\n",
        "batch_size = args.batch_size\n",
        "\n",
        "\n",
        "train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train_neg.csv'))\n",
        "#clear the negative-co-temp folder:\n",
        "if os.path.exists(os.path.join(DATA_DIR, 'negative-co-temp')):\n",
        "    for f in glob.glob(os.path.join(DATA_DIR, 'negative-co-temp', '*.npy')):\n",
        "        os.remove(f)\n",
        "\n",
        "\n",
        "t1 = time.time()\n",
        "print ('Generating item item negative_co-occurrence matrix')\n",
        "start_idx = range(0, n_users, batch_size)\n",
        "list_start_idx = list(start_idx)\n",
        "end_idx = list_start_idx[1:] + [n_users]\n",
        "Parallel(n_jobs=1)(delayed(_coord_batch_n)(lo, hi, train_data, prefix = 'item') for lo, hi in zip(start_idx, end_idx))\n",
        "t2 = time.time()\n",
        "print ('Time : %d seconds'%(t2-t1))\n",
        "pass\n",
        "\n",
        "\n",
        "def _load_coord_matrix(start_idx, end_idx, nrow, ncol, prefix = 'item'):\n",
        "    X = sparse.csr_matrix((nrow, ncol), dtype='float32')\n",
        "\n",
        "    for lo, hi in zip(start_idx, end_idx):\n",
        "        coords = np.load(os.path.join(DATA_DIR, 'negative-co-temp', 'negative_%s_coo_%d_%d.npy' % (prefix, lo, hi)))\n",
        "\n",
        "        rows = coords[:, 0]\n",
        "        cols = coords[:, 1]\n",
        "\n",
        "        tmp = sparse.coo_matrix((np.ones_like(rows), (rows, cols)), shape=(nrow, ncol), dtype='float32').tocsr()\n",
        "        X = X + tmp\n",
        "\n",
        "        print(\"%s %d to %d finished\" % (prefix, lo, hi))\n",
        "        sys.stdout.flush()\n",
        "    return X\n",
        "\n",
        "\n",
        "X, Y = None, None\n",
        "print ('Loading item item negative_co-occurrence matrix and saving to pickle file for fast loading')\n",
        "t1 = time.time()\n",
        "start_idx = range(0, n_users, batch_size)\n",
        "list_start_idx = list(start_idx)\n",
        "end_idx = list_start_idx[1:] + [n_users]\n",
        "X = _load_coord_matrix(start_idx, end_idx, n_items, n_items, prefix = 'item') #item item co-occurrence matrix\n",
        "print ('dumping matrix ...')\n",
        "save_pickle(X, os.path.join(DATA_DIR,'negative_item_item_cooc.dat'))\n",
        "t2 = time.time()\n",
        "print ('Time : %d seconds'%(t2-t1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Z4MgUm5KVd",
        "outputId": "325b281a-3d2e-424c-be71-7255e3ee4836"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating item item negative_co-occurrence matrix\n",
            "Time : 6 seconds\n",
            "Loading item item negative_co-occurrence matrix and saving to pickle file for fast loading\n",
            "item 0 to 5000 finished\n",
            "item 5000 to 10000 finished\n",
            "item 10000 to 15000 finished\n",
            "item 15000 to 20000 finished\n",
            "item 20000 to 20279 finished\n",
            "dumping matrix ...\n",
            "Saved drive/MyDrive/dl2022/negative_item_item_cooc.dat..\n",
            "Time : 0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(\"Description: Running multi-embedding recommendation - RME model\")\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument('--data_path', default='data', type=str, help='path to the data')\n",
        "parser.add_argument('--saved_model_path', default='MODELS', type=str, help='path to save the optimal learned parameters')\n",
        "parser.add_argument('--s', default=1, type=int, help='a pre-defined shifted value for measuring SPPMI')\n",
        "parser.add_argument('--model', default='rme', type=str, help='the model to run: rme, cofactor')\n",
        "parser.add_argument('--n_factors', default=40, type=int, help='number of hidden factors for user/item representation')\n",
        "parser.add_argument('--reg', default=1.0, type=float, help='regularization for user and item latent factors (alpha, beta)')\n",
        "parser.add_argument('--reg_embed', default=1.0, type=float, help='regularization for user and item context latent factors (gamma, delta, theta)')\n",
        "parser.add_argument('--dataset', default=\"ml10m\", type=str, help='dataset')\n",
        "parser.add_argument('--neg_item_inference', default=0, type=int, help='if there is no available disliked items, set this to 1 to infer '\n",
        "                                                                      'negative items for users using our user-oriented EM like algorithm')\n",
        "parser.add_argument('--neg_sample_ratio', default=0.2, type=float, help='negative sample ratio per user. If a user consumed 10 items, and this'\n",
        "                                                                        'neg_sample_ratio = 0.2 --> randomly sample 2 negative items for the user')\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "vM6xjxvo5KYC"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR =  os.path.join('drive', 'MyDrive', 'dl2022')\n",
        "gc.DATA_DIR = DATA_DIR\n",
        "gc.SAVED_MODLE_DIR = args.saved_model_path\n",
        "gc.PRED_DIR = os.path.join(DATA_DIR, 'prediction-temp')\n",
        "SHIFTED_K_VALUE = args.s\n",
        "NEGATIVE_SAMPLE_RATIO = args.neg_sample_ratio\n",
        "save_dir = os.path.join(DATA_DIR, 'model_tmp_res')\n",
        "n_components = args.n_factors\n",
        "lam = args.reg\n",
        "lam_emb = args.reg_embed\n",
        "\n",
        "unique_uid = list()\n",
        "with open(os.path.join(DATA_DIR, 'unique_uid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_uid.append(line.strip())\n",
        "\n",
        "unique_movieId = list()\n",
        "with open(os.path.join(DATA_DIR, 'unique_sid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_movieId.append(line.strip())\n",
        "n_items = len(unique_movieId)\n",
        "n_users = len(unique_uid)\n",
        "n_items = len(unique_movieId)\n",
        "print(n_users, n_items)\n",
        "\n",
        "def load_data(csv_file, shape=(n_users, n_items)):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    rows, cols = np.array(tp['userId']), np.array(tp['movieId'])\n",
        "    seq = np.concatenate((rows[:, None], cols[:, None], np.ones((rows.size, 1), dtype='int')), axis=1)\n",
        "    data = sparse.csr_matrix((np.ones_like(rows), (rows, cols)), dtype=np.int16, shape=shape)\n",
        "    return data, seq, tp\n",
        "\n",
        "def get_row(M, i):\n",
        "    # get the row i of sparse matrix:\n",
        "    lo, hi = M.indptr[i], M.indptr[i + 1]\n",
        "    return lo, hi, M.data[lo:hi], M.indices[lo:hi]\n",
        "\n",
        "\n",
        "def convert_to_SPPMI_matrix(M, max_row, shifted_K=1):\n",
        "    # if we sum the co-occurrence matrix by row wise or column wise --> we have an array that contain the #(i) values\n",
        "    obj_counts = np.asarray(M.sum(axis=1)).ravel()\n",
        "    total_obj_pairs = M.data.sum()\n",
        "    M_sppmi = M.copy()\n",
        "    for i in xrange(max_row):\n",
        "        lo, hi, data, indices = get_row(M, i)\n",
        "        M_sppmi.data[lo:hi] = np.log(data * total_obj_pairs / (obj_counts[i] * obj_counts[indices]))\n",
        "    M_sppmi.data[M_sppmi.data < 0] = 0\n",
        "    M_sppmi.eliminate_zeros()\n",
        "    if shifted_K == 1:\n",
        "        return M_sppmi\n",
        "    else:\n",
        "        M_sppmi.data -= np.log(shifted_K)\n",
        "        M_sppmi.data[M_sppmi.data < 0] = 0\n",
        "        M_sppmi.eliminate_zeros()\n",
        "    return M_sppmi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek9FRi7q5Kao",
        "outputId": "93ccd2af-72d2-4763-f107-d74f6c34ed31"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20279 6051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize with WMF:\n",
        "import wmf\n",
        "import pyll\n",
        "from scipy import sparse\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import produce_negative_embedding as pne # produce_negative_embedding 이 패키지가 없음..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "a1glt4i2eGNy",
        "outputId": "93d8b4a2-cc16-4ab3-8183-ee122b70d073"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-d88181cc9bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mproduce_negative_embedding\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'produce_negative_embedding'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install produce_negative_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0WRzI1CPZCG",
        "outputId": "beab1d89-7785-4232-c726-433989be209b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement produce_negative_embedding (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for produce_negative_embedding\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.neg_item_inference:\n",
        "    #initialize with WMF:\n",
        "    import wmf\n",
        "    import rec_eval\n",
        "    from scipy import sparse\n",
        "    import produce_negative_embedding as pne\n",
        "    import glob\n",
        "    import os\n",
        "\n",
        "    def softmax(x):\n",
        "        \"\"\"Compute softmax values for each ranked list.\"\"\"\n",
        "        # We want the item with higher ranking score have lower prob to be withdrawn as negative instances#\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "    def compute_neg_prob(ranks):\n",
        "        return softmax(np.negative(ranks))\n",
        "    def _make_prediction(train_data, Et, Eb, user_idx, batch_users, mu=None,\n",
        "                         vad_data=None):\n",
        "        n_songs = train_data.shape[1]\n",
        "        # exclude examples from training and validation (if any)\n",
        "        item_idx = np.zeros((batch_users, n_songs), dtype=bool)\n",
        "        item_idx[train_data[user_idx].nonzero()] = True\n",
        "        if vad_data is not None:\n",
        "            item_idx[vad_data[user_idx].nonzero()] = True\n",
        "        X_pred = Et[user_idx].dot(Eb)\n",
        "        if mu is not None:\n",
        "            if isinstance(mu, np.ndarray):\n",
        "                assert mu.size == n_songs  # mu_i\n",
        "                X_pred *= mu\n",
        "            elif isinstance(mu, dict):  # func(mu_ui)\n",
        "                params, func = mu['params'], mu['func']\n",
        "                args = [params[0][user_idx], params[1]]\n",
        "                if len(params) > 2:  # for bias term in document or length-scale\n",
        "                    args += [params[2][user_idx]]\n",
        "                if not callable(func):\n",
        "                    raise TypeError(\"expecting a callable function\")\n",
        "                X_pred *= func(*args)\n",
        "            else:\n",
        "                raise ValueError(\"unsupported mu type\")\n",
        "        X_pred[item_idx] = np.inf\n",
        "        return X_pred\n",
        "\n",
        "    def gen_neg_instances(train_data, U, VT, user_idx, neg_ratio = 1.0, iter = 0):\n",
        "        print ('Job start... %d to %d'%(user_idx.start, user_idx.stop))\n",
        "        #if user_idx.start != 99000: return\n",
        "        batch_users = user_idx.stop - user_idx.start\n",
        "        X_pred = _make_prediction(train_data, U, VT, user_idx, batch_users, vad_data=vad_data)\n",
        "\n",
        "        rows = []\n",
        "        cols = []\n",
        "        total_lost = 0\n",
        "        for idx, uid in enumerate(range(user_idx.start, user_idx.stop)):\n",
        "            num_pos = train_data[uid].count_nonzero()\n",
        "            num_neg = int(num_pos * neg_ratio)\n",
        "            if num_neg <= 0: continue\n",
        "            ranks = X_pred[idx]\n",
        "            neg_withdrawn_prob = compute_neg_prob(ranks)\n",
        "            # print (neg_withdrawn_prob)\n",
        "            neg_instances = list(set(np.random.choice(range(n_items), num_neg, p = neg_withdrawn_prob)))\n",
        "            #rows = rows + len(neg_instances)*[uid]\n",
        "            #uid_dup = np.empty(len(neg_instances))\n",
        "            #uid_dup.fill(uid)\n",
        "            if uid < 0: print ('error with %d to %d'%(user_idx.start, user_idx.stop))\n",
        "            #rows = rows + uid_dup\n",
        "            rows = np.append(rows, np.full( len(neg_instances), uid )  )\n",
        "            cols = np.append(cols, neg_instances)\n",
        "        # print 'check for neg values: ', np.sum(rows <0)\n",
        "        # print 'check for neg values: ', np.sum(cols <0)\n",
        "        if  len(rows) > 0:\n",
        "            path = os.path.join(DATA_DIR, 'sub_dataframe_iter_%d_idxstart_%d.csv' % (iter, user_idx.start))\n",
        "            assert len(rows) == len(cols)\n",
        "            with open(path, 'w') as writer:\n",
        "                for i in range(len(rows)): writer.write(str(rows[i]) + \",\" + str(cols[i]) + '\\n')\n",
        "                writer.flush()\n",
        "            #df = pd.DataFrame({'uid':rows, 'sid':cols}, columns=[\"uid\", \"sid\"], dtype=np.int16)\n",
        "            #df.to_csv(path, sep=\",\",header=False, index = False)\n",
        "        # return df\n",
        "    U, V = None, None\n",
        "\n",
        "\n",
        "\n",
        "    vad_data, vad_raw, vad_df = load_data(os.path.join(DATA_DIR, 'validation.csv'))\n",
        "    train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train.csv'))\n",
        "    test_data, test_raw, test_df = load_data(os.path.join(DATA_DIR, 'test.csv'))\n",
        "    U, V = wmf.decompose(train_data, vad_data, num_factors= n_components)\n",
        "    VT = V.T\n",
        "    iter, max_iter = 0, 10\n",
        "\n",
        "    #load postivie information\n",
        "    X = load_pickle(os.path.join(DATA_DIR, 'item_item_cooc.dat'))\n",
        "    Y = load_pickle(os.path.join(DATA_DIR, 'user_user_cooc.dat'))\n",
        "    X_sppmi = convert_to_SPPMI_matrix(X, max_row=n_items, shifted_K=SHIFTED_K_VALUE)\n",
        "    Y_sppmi = convert_to_SPPMI_matrix(Y, max_row=n_users, shifted_K=SHIFTED_K_VALUE)\n",
        "\n",
        "    best_ndcg100 = 0.0\n",
        "    best_iter = 1\n",
        "    early_stopping = False\n",
        "    while (iter < max_iter and not early_stopping):\n",
        "        ################ Expectation step: ######################\n",
        "        user_slices = rec_eval.user_idx_generator(n_users, batch_users=5000)\n",
        "        print ('GENERATING NEGATIVE INSTANCES ...')\n",
        "        t1 = time.time()\n",
        "        df = Parallel(n_jobs=16)(delayed(gen_neg_instances)(train_data, U, VT, user_idx, neg_ratio = NEGATIVE_SAMPLE_RATIO, iter = iter)\n",
        "                                      for user_idx in user_slices)\n",
        "        t2 = time.time()\n",
        "        print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "\n",
        "        print ('merging to one file ...')\n",
        "        t1 = time.time()\n",
        "        neg_file_out = os.path.join(DATA_DIR, 'train_neg_iter_%d.csv' % (iter))\n",
        "        with open(neg_file_out, 'w') as writer:\n",
        "            writer.write('userId,movieId\\n')\n",
        "        # os.system(\"echo uid,sid >> \" + neg_file_out)\n",
        "        for f in glob.glob(os.path.join(DATA_DIR, 'sub_dataframe_iter*')):\n",
        "            os.system(\"cat \" + f + \" >> \" + neg_file_out)\n",
        "                # with open(f, 'rb') as reader:\n",
        "                #\n",
        "                #     writer.write(reader.readline())\n",
        "            # writer.flush()\n",
        "        #clean\n",
        "        for f in glob.glob(os.path.join(DATA_DIR, 'sub_dataframe_iter*')):\n",
        "            os.remove(f)\n",
        "\n",
        "        t2 = time.time()\n",
        "        print ('Time : %d seconds' % (t2 - t1))\n",
        "        # neg_train_df = pd.concat(df)\n",
        "        # neg_train_df.to_csv(neg_file_out, index = False)\n",
        "        #########################################################\n",
        "\n",
        "        ################ maximization step:######################\n",
        "        print ('GENERATING NEGATIVE EMBEDDINGS ...')\n",
        "        t1 = time.time()\n",
        "        train_neg_data, _, train_neg_df = load_data(neg_file_out, shape=(n_users, n_items))\n",
        "        #build the negative info:\n",
        "        X_neg, _ = pne.produce_neg_embeddings(DATA_DIR, train_neg_data, n_users, n_items, iter = iter)\n",
        "        X_neg_sppmi = convert_to_SPPMI_matrix(X_neg, max_row=n_items, shifted_K=SHIFTED_K_VALUE)\n",
        "        Y_neg_sppmi = None\n",
        "        t2 = time.time()\n",
        "        print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "        # build the model\n",
        "        print ('build the model...')\n",
        "        t1 = time.time()\n",
        "        runner = ModelRunner(train_data, vad_data, None, X_sppmi, X_neg_sppmi, Y_sppmi, None, save_dir=save_dir)\n",
        "        U, V, ndcg100 = runner.run(\"rme\", n_jobs = 1,\n",
        "                                         lam=lam, lam_emb=lam_emb, n_components = n_components, ret_params_only = 1)\n",
        "        t2 = time.time()\n",
        "        print ('Time : %d seconds' % (t2 - t1))\n",
        "        print ('*************************************ITER %d ******************************************' % iter)\n",
        "        print ('NDCG@100 at this iter:',ndcg100)\n",
        "        #\n",
        "        if best_ndcg100 < ndcg100:\n",
        "            best_iter = iter\n",
        "            best_ndcg100 = ndcg100\n",
        "        else:\n",
        "            early_stopping = True\n",
        "        iter += 1\n",
        "        #########################################################\n",
        "    print ('Max NDCG@100: %f , at iter: %d'%(best_ndcg100, best_iter))\n",
        "    best_train_neg_file = os.path.join(DATA_DIR, 'train_neg_iter_%d.csv' % (best_iter))\n",
        "    best_train_neg_file_newname = os.path.join(DATA_DIR, 'train_neg.csv')\n",
        "    best_train_emb_file = os.path.join(DATA_DIR, 'negative_item_item_cooc_iter%d.dat' % (best_iter))\n",
        "    best_train_emb_file_newname = os.path.join(DATA_DIR, 'negative_item_item_cooc.dat')\n",
        "    print ('renaming from %s to %s'%(best_train_neg_file, best_train_neg_file_newname))\n",
        "    os.rename(best_train_neg_file, best_train_neg_file_newname)\n",
        "    print ('renaming from %s to %s' % (best_train_emb_file, best_train_emb_file_newname))\n",
        "    os.rename(best_train_emb_file, best_train_emb_file_newname)\n",
        "    #cleaning\n",
        "    for i in range(max_iter):\n",
        "        if i == best_iter: continue\n",
        "        if early_stopping and (i > best_iter + 1): break\n",
        "        del_file = os.path.join(DATA_DIR, 'train_neg_iter_%d.csv' % ( i))\n",
        "        os.remove(del_file)\n",
        "        del_file = os.path.join(DATA_DIR, 'negative_item_item_cooc_iter%d.dat' % (i))\n",
        "        os.remove(del_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LOAD_NEGATIVE_MATRIX = True\n",
        "if args.model.lower() != 'rme':\n",
        "    LOAD_NEGATIVE_MATRIX = False\n",
        "recalls = np.zeros(5, dtype=np.float32) #store results of topk recommendation in range [5, 10, 20, 50, 100]\n",
        "ndcgs = np.zeros(5, dtype=np.float32)\n",
        "maps = np.zeros(5, dtype=np.float32)\n",
        "print ('*************************************lam =  %.3f ******************************************' % lam)\n",
        "print ('*************************************lam embedding =  %.3f ******************************************' % lam_emb)\n",
        "\n",
        "# train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train_fold%d.csv'%FOLD))\n",
        "vad_data, vad_raw, vad_df = load_data(os.path.join(DATA_DIR, 'validation.csv'))\n",
        "test_data, test_raw, test_df = load_data(os.path.join(DATA_DIR, 'test.csv'))\n",
        "train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train.csv' ))\n",
        "\n",
        "print ('loading pro_pro_cooc.dat')\n",
        "t1 = time.time()\n",
        "X = load_pickle(os.path.join(DATA_DIR, 'item_item_cooc.dat'))\n",
        "t2 = time.time()\n",
        "print ('[INFO]: sparse matrix size of item item co-occurrence matrix: %d mb\\n' % (\n",
        "    (X.data.nbytes + X.indices.nbytes + X.indptr.nbytes) / (1024 * 1024)))\n",
        "print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "print ('loading user_user_cooc.dat')\n",
        "t1 = time.time()\n",
        "Y = load_pickle(os.path.join(DATA_DIR, 'user_user_cooc.dat'))\n",
        "t2 = time.time()\n",
        "print ('[INFO]: sparse matrix size of user user co-occurrence matrix: %d mb\\n' % (\n",
        "    (Y.data.nbytes + Y.indices.nbytes + Y.indptr.nbytes) / (1024 * 1024)))\n",
        "print ('Time : %d seconds' % (t2 - t1))\n",
        "################# LOADING NEGATIVE CO-OCCURRENCE MATRIX ########################################\n",
        "\n",
        "if LOAD_NEGATIVE_MATRIX:\n",
        "    print ('test loading negative_pro_pro_cooc.dat')\n",
        "    t1 = time.time()\n",
        "    X_neg = load_pickle(os.path.join(DATA_DIR, 'negative_item_item_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('[INFO]: sparse matrix size of negative item item co-occurrence matrix: %d mb\\n' % (\n",
        "        (X_neg.data.nbytes + X_neg.indices.nbytes + X_neg.indptr.nbytes) / (1024 * 1024)))\n",
        "    print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "########## converting CO-OCCURRENCE MATRIX INTO Shifted Positive Pointwise Mutual Information (SPPMI) matrix ###########\n",
        "####### We already know the user-user co-occurrence matrix Y and item-item co-occurrence matrix X\n",
        "\n",
        "print ('converting co-occurrence matrix into sppmi matrix')\n",
        "t1 = time.time()\n",
        "X_sppmi = convert_to_SPPMI_matrix(X, max_row=n_items, shifted_K=SHIFTED_K_VALUE)\n",
        "Y_sppmi = convert_to_SPPMI_matrix(Y, max_row=n_users, shifted_K=SHIFTED_K_VALUE)\n",
        "t2 = time.time()\n",
        "print ('Time : %d seconds' % (t2 - t1))\n",
        "# if DEBUG_MODE:\n",
        "#     print 'item sppmi matrix'\n",
        "#     print X_sppmi\n",
        "#     print 'user sppmi matrix'\n",
        "#     print Y_sppmi\n",
        "\n",
        "############### Negative SPPMI matrix ##########################\n",
        "X_neg_sppmi = None\n",
        "Y_neg_sppmi = None\n",
        "if LOAD_NEGATIVE_MATRIX:\n",
        "    print ('converting negative co-occurrence matrix into sppmi matrix')\n",
        "    t1 = time.time()\n",
        "    X_neg_sppmi = convert_to_SPPMI_matrix(X_neg, max_row=n_items, shifted_K=SHIFTED_K_VALUE)\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds' % (t2 - t1))\n",
        "################################################################\n",
        "\n",
        "\n",
        "######## Finally, we have train_data, vad_data, test_data,\n",
        "# X_sppmi: item item Shifted Positive Pointwise Mutual Information matrix\n",
        "# Y_sppmi: user-user       Shifted Positive Pointwise Mutual Information matrix\n",
        "\n",
        "\n",
        "print ('Training data', train_data.shape)\n",
        "print ('Validation data', vad_data.shape)\n",
        "print ('Testing data', test_data.shape)\n",
        "\n",
        "n_jobs = 1  # default value\n",
        "model_type = 'model2'  # default value\n",
        "if os.path.exists(save_dir):\n",
        "    #clearning folder\n",
        "    lst = glob.glob(os.path.join(save_dir, '*.*'))\n",
        "    for f in lst:\n",
        "        os.remove(f)\n",
        "else:\n",
        "    os.mkdir(save_dir)\n",
        "\n",
        "\n",
        "runner = ModelRunner(train_data, vad_data, test_data, X_sppmi, X_neg_sppmi, Y_sppmi, Y_neg_sppmi,\n",
        "                       save_dir=save_dir)\n",
        "\n",
        "start = time.time()\n",
        "if args.model == 'wmf':\n",
        "    (recalls, ndcgs, maps) = runner.run(\"wmf\", n_jobs=n_jobs, lam=lam,\n",
        "                                                         saved_model = True,\n",
        "                                                         n_components = n_components)\n",
        "if args.model == 'cofactor':\n",
        "    (recalls, ndcgs, maps) = runner.run(\"cofactor\", n_jobs=n_jobs,\n",
        "                                                        lam=lam,\n",
        "                                                         saved_model=True,\n",
        "                                                         n_components=n_components)\n",
        "if args.model == 'rme':\n",
        "    (recalls, ndcgs, maps) = runner.run(\"rme\", n_jobs=n_jobs,lam=lam, lam_emb = lam_emb,\n",
        "                                                         saved_model=True,\n",
        "                                                         n_components=n_components)\n",
        "end = time.time()\n",
        "print ('total running time: %d seconds'%(end-start))\n",
        "for idx, topk in enumerate([5, 10, 20, 50, 100]):\n",
        "    print ('top-%d results: recall@%d = %.4f, ndcg@%d = %.4f, map@%d = %.4f'%(topk,\n",
        "                                                                                  topk, recalls[idx],\n",
        "                                                                                  topk, ndcgs[idx],\n",
        "                                                                                  topk, maps[idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "KYEzhKNoeNDU",
        "outputId": "2f90b94a-fa1c-4331-f853-96a6e2dcc595"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************************************lam =  1.000 ******************************************\n",
            "*************************************lam embedding =  1.000 ******************************************\n",
            "loading pro_pro_cooc.dat\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-c78d4c818cd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m print ('[INFO]: sparse matrix size of item item co-occurrence matrix: %d mb\\n' % (\n\u001b[0;32m--> 200\u001b[0;31m     (X.data.nbytes + X.indices.nbytes + X.indptr.nbytes) / (1024 * 1024)))\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Time : %d seconds'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load_pickle 했을 때 왜 none값을 갖고 오는지... "
      ],
      "metadata": {
        "id": "BRkUFSAB5KfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 논문에서는 아래와 같은 결과를 엉었음.\n",
        "\n",
        "\"\"\"\n",
        "top-5 results: recall@5 = 0.1559, ndcg@5 = 0.1613, map@5 = 0.1076\n",
        "top-10 results: recall@10 = 0.1513, ndcg@10 = 0.1547, map@10 = 0.0851\n",
        "top-20 results: recall@20 = 0.1477, ndcg@20 = 0.1473, map@20 = 0.0669\n",
        "top-50 results: recall@50 = 0.1819, ndcg@50 = 0.1553, map@50 = 0.0562\n",
        "top-100 results: recall@100 = 0.2533, ndcg@100 = 0.1825, map@100 = 0.0579\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "J9NiiwjYuN_7",
        "outputId": "08abf395-d09a-43b8-8d74-d5ae71d71e4d"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntop-5 results: recall@5 = 0.1559, ndcg@5 = 0.1613, map@5 = 0.1076\\ntop-10 results: recall@10 = 0.1513, ndcg@10 = 0.1547, map@10 = 0.0851\\ntop-20 results: recall@20 = 0.1477, ndcg@20 = 0.1473, map@20 = 0.0669\\ntop-50 results: recall@50 = 0.1819, ndcg@50 = 0.1553, map@50 = 0.0562\\ntop-100 results: recall@100 = 0.2533, ndcg@100 = 0.1825, map@100 = 0.0579\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMYibry3QICF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}